# Plan DÃ©taillÃ© - AmÃ©lioration Ultra-AvancÃ©e de la DÃ©tection d'Instruments de Batterie

## ğŸ“Š Ã‰tat Actuel (Progress)

### âœ… TerminÃ©
- [x] Ajout des dÃ©pendances TensorFlow-Magenta et nouvelles librairies
- [x] Mise Ã  jour requirements.txt avec magenta==2.1.4, tensorflow-probability==0.19.0, pretty_midi==0.2.10, scikit-learn==1.2.2, mir-eval==0.7
- [x] Ajout des imports nÃ©cessaires (sklearn, scipy, warnings)

### ğŸ”„ En Cours
- [ ] ImplÃ©mentation AdvancedFeatureExtractor avec MFCC, spectral contrast, chroma, tonnetz

### ğŸ“‹ Ã€ Faire
- [ ] CrÃ©er MagentaDrumClassifier pour intÃ©grer le modÃ¨le OaF Drums
- [ ] DÃ©velopper HybridDrumClassifier combinant plusieurs approches
- [ ] IntÃ©grer le systÃ¨me hybride dans le pipeline principal
- [ ] Tester et valider les amÃ©liorations de performance

## ğŸ¯ Objectifs du Projet

### ProblÃ¨me IdentifiÃ©
L'approche actuelle basÃ©e uniquement sur l'analyse FFT et les bandes de frÃ©quence est insuffisante :
- Classification imprÃ©cise des instruments (snare ~40%, hi-hat ~30%, toms ~20%)
- Pas de distinction entre hi-hat open/close
- Mauvaise dÃ©tection des nuances et vÃ©locitÃ©
- Timing approximatif des onsets

### Objectifs de Performance
- **Kick Detection** : 90%+ (vs ~60% actuel)
- **Snare Detection** : 85%+ (vs ~40% actuel)
- **Hi-hat Classification** : 80%+ (vs ~30% actuel)
- **Toms/Cymbales** : 75%+ (vs ~20% actuel)

## ğŸ—ï¸ Architecture du SystÃ¨me

### 1. AdvancedFeatureExtractor
```python
class AdvancedFeatureExtractor:
    def __init__(self, sr=44100, n_mfcc=13, n_chroma=12):
        self.sr = sr
        self.n_mfcc = n_mfcc
        self.n_chroma = n_chroma
        self.scaler = StandardScaler()
        
    def extract_features(self, audio_window):
        # MFCC (13 coefficients)
        # Spectral Contrast (7 features)
        # Chroma Features (12 features)
        # Tonnetz (6 features)
        # Spectral Statistics (centroÃ¯de, rolloff, flatness, bandwidth)
        # Temporal Features (ZCR, RMS, attack/decay)
```

### 2. MagentaDrumClassifier
```python
class MagentaDrumClassifier:
    def __init__(self):
        self.model = None  # ModÃ¨le Magenta OaF Drums
        self.confidence_threshold = 0.7
        
    def load_model(self):
        # Charger le modÃ¨le prÃ©-entraÃ®nÃ© OaF Drums
        
    def classify_onsets(self, drum_audio, onset_times):
        # Utiliser OaF Drums pour classification
        # Retourner classe + confidence + vÃ©locitÃ©
```

### 3. HybridDrumClassifier
```python
class HybridDrumClassifier:
    def __init__(self):
        self.feature_extractor = AdvancedFeatureExtractor()
        self.magenta_classifier = MagentaDrumClassifier()
        self.weights = {
            'magenta': 0.4,
            'features': 0.3,
            'context': 0.3
        }
        
    def classify_onset(self, onset_time, drum_audio):
        # Combiner les 3 approches avec vote pondÃ©rÃ©
        # Validation croisÃ©e des rÃ©sultats
```

## ğŸ“ Plan d'ImplÃ©mentation DÃ©taillÃ©

### Phase 1 : AdvancedFeatureExtractor (EN COURS)

#### 1.1 Structure de Base
```python
class AdvancedFeatureExtractor:
    def __init__(self, sr=44100, n_mfcc=13, n_chroma=12, n_contrast=7):
        self.sr = sr
        self.n_mfcc = n_mfcc
        self.n_chroma = n_chroma
        self.n_contrast = n_contrast
        self.scaler = StandardScaler()
        self.feature_cache = {}
```

#### 1.2 MÃ©thodes d'Extraction
- `extract_mfcc(audio_window)` : 13 coefficients MFCC
- `extract_spectral_contrast(audio_window)` : 7 features de contraste spectral
- `extract_chroma(audio_window)` : 12 features chromatiques
- `extract_tonnetz(audio_window)` : 6 features tonnetz
- `extract_spectral_stats(audio_window)` : centroÃ¯de, rolloff, flatness, bandwidth
- `extract_temporal_features(audio_window)` : ZCR, RMS, attack/decay

#### 1.3 MÃ©thode Principale
```python
def extract_comprehensive_features(self, audio_window):
    features = {}
    
    # MFCC (13 features)
    features['mfcc'] = self.extract_mfcc(audio_window)
    
    # Spectral Contrast (7 features)
    features['spectral_contrast'] = self.extract_spectral_contrast(audio_window)
    
    # Chroma (12 features)
    features['chroma'] = self.extract_chroma(audio_window)
    
    # Tonnetz (6 features)
    features['tonnetz'] = self.extract_tonnetz(audio_window)
    
    # Spectral Statistics (4 features)
    features['spectral_stats'] = self.extract_spectral_stats(audio_window)
    
    # Temporal Features (5 features)
    features['temporal'] = self.extract_temporal_features(audio_window)
    
    # Combiner toutes les features (47 features total)
    combined_features = np.concatenate([
        features['mfcc'],
        features['spectral_contrast'],
        features['chroma'],
        features['tonnetz'],
        features['spectral_stats'],
        features['temporal']
    ])
    
    return combined_features, features
```

### Phase 2 : MagentaDrumClassifier

#### 2.1 Installation et Configuration
```python
# Dans requirements.txt (dÃ©jÃ  fait)
magenta==2.1.4
tensorflow-probability==0.19.0
pretty_midi==0.2.10
```

#### 2.2 IntÃ©gration du ModÃ¨le
```python
class MagentaDrumClassifier:
    def __init__(self):
        self.model = None
        self.confidence_threshold = 0.7
        self.drum_classes = {
            0: 'kick',
            1: 'snare',
            2: 'hi-hat-close',
            3: 'hi-hat-open',
            4: 'tom-high',
            5: 'tom-low',
            6: 'tom-floor',
            7: 'crash',
            8: 'ride',
            9: 'ride-bell'
        }
        
    def load_model(self):
        # Charger le modÃ¨le OaF Drums prÃ©-entraÃ®nÃ©
        try:
            from magenta.models.onsets_frames_transcription import model
            self.model = model.load_model()
        except ImportError:
            print("Magenta not available, using fallback")
            self.model = None
            
    def classify_onsets(self, drum_audio, onset_times):
        if self.model is None:
            return self.fallback_classification(drum_audio, onset_times)
            
        # Utiliser le modÃ¨le Magenta pour classification
        predictions = self.model.predict(drum_audio)
        
        # Mapper les onsets aux prÃ©dictions
        classified_onsets = []
        for onset_time in onset_times:
            prediction = self.get_prediction_at_time(predictions, onset_time)
            if prediction['confidence'] > self.confidence_threshold:
                classified_onsets.append({
                    'time': onset_time,
                    'instrument': prediction['instrument'],
                    'confidence': prediction['confidence'],
                    'velocity': prediction['velocity']
                })
                
        return classified_onsets
```

### Phase 3 : HybridDrumClassifier

#### 3.1 SystÃ¨me de Vote PondÃ©rÃ©
```python
class HybridDrumClassifier:
    def __init__(self):
        self.feature_extractor = AdvancedFeatureExtractor()
        self.magenta_classifier = MagentaDrumClassifier()
        self.weights = {
            'magenta': 0.4,
            'features': 0.3,
            'context': 0.3
        }
        self.confidence_threshold = 0.6
        
    def classify_onset(self, onset_time, drum_audio, context_window=0.1):
        # Extraire fenÃªtre audio autour de l'onset
        window = self.extract_audio_window(drum_audio, onset_time, context_window)
        
        # Approche 1: Magenta OaF Drums
        magenta_result = self.magenta_classifier.classify_single_onset(window)
        
        # Approche 2: Features avancÃ©es + ML
        features = self.feature_extractor.extract_comprehensive_features(window)
        features_result = self.classify_with_features(features)
        
        # Approche 3: Analyse contextuelle
        context_result = self.analyze_context(onset_time, drum_audio)
        
        # Vote pondÃ©rÃ©
        final_result = self.weighted_vote(magenta_result, features_result, context_result)
        
        return final_result
```

#### 3.2 Classification par Features
```python
def classify_with_features(self, features):
    # Utiliser un modÃ¨le ML entraÃ®nÃ© sur les features
    # Ou utiliser des rÃ¨gles basÃ©es sur les features
    
    mfcc = features['mfcc']
    spectral_contrast = features['spectral_contrast']
    chroma = features['chroma']
    tonnetz = features['tonnetz']
    spectral_stats = features['spectral_stats']
    temporal = features['temporal']
    
    # RÃ¨gles de classification basÃ©es sur les features
    if temporal['attack_time'] < 0.01 and spectral_stats['centroid'] < 200:
        return {'instrument': 'kick', 'confidence': 0.8}
    elif spectral_contrast[2] > 0.5 and temporal['zcr'] > 0.1:
        return {'instrument': 'snare', 'confidence': 0.7}
    # ... autres rÃ¨gles
```

### Phase 4 : IntÃ©gration dans le Pipeline

#### 4.1 Modification de la Classe AudioToChart
```python
class AudioToChart:
    def __init__(self, input_audio_path, use_original_bgm=True):
        # ... existing code ...
        self.hybrid_classifier = HybridDrumClassifier()
        
    def improved_instrument_classification(self, fused_onsets):
        """Classification ultra-amÃ©liorÃ©e des instruments"""
        print("Classification ultra-amÃ©liorÃ©e des instruments...")
        
        classified_onsets = [[] for _ in range(self.num_class)]
        
        for onset_time in fused_onsets:
            # Utiliser le systÃ¨me hybride
            result = self.hybrid_classifier.classify_onset(
                onset_time, 
                self.drum_audio
            )
            
            if result['confidence'] > 0.6:
                instrument_class = self.map_instrument_to_class(result['instrument'])
                classified_onsets[instrument_class].append({
                    'time': onset_time,
                    'confidence': result['confidence'],
                    'velocity': result.get('velocity', 0.7)
                })
        
        return classified_onsets
```

#### 4.2 Nouveau Pipeline
```python
def extract_beats(self):
    """Pipeline amÃ©liorÃ© avec classification hybride"""
    print("Starting ultra-improved hybrid onset detection pipeline...")
    
    # Ã‰tapes 1-3: Audio separation, tempo, ML model (inchangÃ©)
    self.separate_audio_tracks()
    self.detect_tempo_and_beats()
    self.load_model()
    model_input = self.preprocess_drums()
    self.predict_onsets(model_input)
    self.peak_picking()
    
    # Ã‰tape 4: Fusion des onsets (inchangÃ©)
    fused_onsets = self.fuse_onset_detections()
    
    # Ã‰tape 5: Classification ultra-amÃ©liorÃ©e (NOUVEAU)
    classified_onsets = self.improved_instrument_classification(fused_onsets)
    
    # Ã‰tape 6: Timing amÃ©liorÃ©
    self.apply_improved_timing(classified_onsets)
    
    print("Ultra-improved hybrid onset detection pipeline completed")
```

## ğŸ§ª Tests et Validation

### Tests Unitaires
```python
class TestAdvancedFeatureExtractor:
    def test_mfcc_extraction(self):
        # Test MFCC extraction
        
    def test_spectral_contrast(self):
        # Test spectral contrast
        
    def test_feature_dimensions(self):
        # VÃ©rifier que les dimensions sont correctes
```

### Tests d'IntÃ©gration
```python
class TestHybridClassifier:
    def test_kick_detection(self):
        # Test sur Ã©chantillons de kick
        
    def test_snare_detection(self):
        # Test sur Ã©chantillons de snare
        
    def test_hihat_classification(self):
        # Test distinction hi-hat open/close
```

### MÃ©triques de Performance
```python
def evaluate_performance(self, ground_truth, predictions):
    # Precision, Recall, F1-Score par instrument
    # Confusion Matrix
    # Accuracy globale
    # Temps de traitement
```

## ğŸ“ Structure des Fichiers

```
audio2dtx/
â”œâ”€â”€ audio_to_chart.py          # Classe principale (modifiÃ©e)
â”œâ”€â”€ advanced_features.py       # AdvancedFeatureExtractor (nouveau)
â”œâ”€â”€ magenta_classifier.py      # MagentaDrumClassifier (nouveau)
â”œâ”€â”€ hybrid_classifier.py       # HybridDrumClassifier (nouveau)
â”œâ”€â”€ requirements.txt           # DÃ©pendances (modifiÃ©)
â”œâ”€â”€ plan.md                   # Ce fichier
â””â”€â”€ tests/
    â”œâ”€â”€ test_features.py       # Tests unitaires
    â”œâ”€â”€ test_magenta.py        # Tests Magenta
    â””â”€â”€ test_hybrid.py         # Tests systÃ¨me hybride
```

## ğŸš€ Prochaines Ã‰tapes

### ImmÃ©diate (Reprendre ici)
1. Finir l'implÃ©mentation de `AdvancedFeatureExtractor`
2. CrÃ©er les mÃ©thodes d'extraction pour chaque type de feature
3. Tester l'extraction de features sur des Ã©chantillons

### Moyen Terme
1. IntÃ©grer le modÃ¨le Magenta OaF Drums
2. CrÃ©er le systÃ¨me de vote pondÃ©rÃ©
3. Tester les performances sur le fichier song.mp3

### Long Terme
1. Optimiser les poids du systÃ¨me hybride
2. Ajouter des mÃ©triques de performance
3. CrÃ©er une interface de debugging pour visualiser les features

## ğŸ”§ Commandes pour Reprendre

```bash
# Construire avec les nouvelles dÃ©pendances
make clean && make build

# Tester le systÃ¨me actuel
make run

# DÃ©velopper les nouvelles features
# Modifier audio_to_chart.py pour ajouter AdvancedFeatureExtractor
```

## ğŸ“Š RÃ©sultats Attendus

### Performance Cible
- **Kick** : 90%+ prÃ©cision
- **Snare** : 85%+ prÃ©cision  
- **Hi-hat** : 80%+ prÃ©cision (avec distinction open/close)
- **Toms** : 75%+ prÃ©cision (avec distinction high/low/floor)
- **Cymbales** : 75%+ prÃ©cision (crash/ride/bell)

### Temps de Traitement
- Maintenir < 2 minutes pour un fichier de 3 minutes
- Optimiser les calculs avec mise en cache des features

### QualitÃ© DTX
- Rythmes plus naturels et musicaux
- Meilleure distribution des instruments
- VÃ©locitÃ© et expressivitÃ© amÃ©liorÃ©es

---

**Note** : Ce plan peut Ãªtre exÃ©cutÃ© Ã©tape par Ã©tape. Chaque phase peut Ãªtre testÃ©e indÃ©pendamment avant de passer Ã  la suivante.